
- [数据倾斜的连锁反应](#数据倾斜的连锁反应)
- [如何定位](#如何定位)
- [数据倾斜场景及解决方案](#数据倾斜场景及解决方案)
  - [场景一：数据源 source 消费不均匀](#场景一数据源-source-消费不均匀)
  - [场景二：key 分布不均匀的无统计场景](#场景二key-分布不均匀的无统计场景)
  - [场景三：key 分布不均匀的统计场景](#场景三key-分布不均匀的统计场景)
  - [场景四：小表关联超大表](#场景四小表关联超大表)


# 数据倾斜的连锁反应

（1）单点问题
数据集中在某些分区上（Subtask），导致数据严重不平衡。

（2）GC 频繁
过多的数据集中在某些 JVM（TaskManager），使得JVM 的内存资源短缺，导致频繁 GC。

（3）吞吐下降、延迟增大
数据单点和频繁 GC 导致吞吐下降、延迟增大，产生**反压**。

（4）系统崩溃
严重情况下，过长的 GC 导致 TaskManager 失联，系统崩溃。


# 如何定位

一般数据倾斜会导致反压，因此可以先定位反压后再定位数据倾斜。

步骤1:定位反压 
定位反压有2种方式：Flink Web UI 自带的反压监控（直接方式）、Flink Task Metrics（间接方式）。通过监控反压的信息，可以获取到数据处理瓶颈的 Subtask。

参考：[如何分析及处理反压?](https://www.jianshu.com/p/c7ecd5683226)

步骤2：确定数据倾斜
Flink Web UI 自带Subtask 接收和发送的数据量。当 Subtasks 之间处理的数据量有较大的差距，则该 Subtask 出现数据倾斜。


#  数据倾斜场景及解决方案

## 场景一：数据源 source 消费不均匀
解决思路：通过调整并发度，解决数据源消费不均匀或者数据源反压的情况。例如kafka数据源，可以调整 KafkaSource 的并发度解决消费不均匀。调整并发度的原则：KafkaSource 并发度与 kafka 分区数是一样的，或者 kafka 分区数是KafkaSource 并发度的整数倍。

## 场景二：key 分布不均匀的无统计场景
说明：key 分布不均匀的无统计场景，例如上游数据分布不均匀，使用keyBy来打散数据。

解决思路： 通过添加随机前缀，打散 key 的分布，使得数据不会集中在几个 Subtask。

具体措施：
① 在原来分区 key/uid 的基础上，加上随机的前缀或者后缀。
② 使用数据到达的顺序seq，作为分区的key。

## 场景三：key 分布不均匀的统计场景
解决思路：聚合统计前，先进行预聚合，例如两阶段聚合（加盐局部聚合+去盐全局聚合）。

两阶段聚合的具体措施：
① 预聚合：加盐局部聚合，在原来的 key 上加随机的前缀或者后缀。
② 聚合：去盐全局聚合，删除预聚合添加的前缀或者后缀，然后进行聚合统计。


## 场景四：小表关联超大表

案例：统计一个网站各个端的每分钟的pv，从kafka消费过来的数据首先会按照端进行分组，然后执行聚合函数count来进行pv的计算。此时如果某一个端产生的数据特别大，比如我们的微信小程序端产生数据远远大于其他app端的数据，那么把这些数据分组到某一个算子之后，由于这个算子的处理速度跟不上，就会产生数据倾斜。示例 sql 如下：

```sql
select TUMBLE_END(proc_time, INTERVAL '1' MINUTE) as winEnd,plat,count(*) as pv  from source_kafka_table 
group by TUMBLE(proc_time, INTERVAL '1' MINUTE) ,plat
```
优化前，在 Flink Web-UI 可以看到数据倾斜的问题：
![数据倾斜优化前示例](../../resources/image/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E7%A4%BA%E4%BE%8B1.png)

解决思路：通过对分组的key加上随机数，再次打散，分别计算打散后不同的分组的pv数，然后在最外层再包一层，把打散的数据再次聚合，这样就解决了数据倾斜的问题。常见的
```sql
select winEnd,split_index(plat1,'_',0) as plat2,sum(pv) from (

  select TUMBLE_END(proc_time, INTERVAL '1' MINUTE) as winEnd,plat1,count(*) as pv from (

    -- 最内层，将分组的key，也就是plat加上一个随机数打散
    select plat || '_' || cast(cast(RAND()*100 as int) as string) as plat1 ,proc_time from source_kafka_table 

) group by TUMBLE(proc_time, INTERVAL '1' MINUTE), plat1

) group by winEnd,split_index(plat1,'_',0)
```

优化后，效果如下：
![数据倾斜优化后示例](../../resources/image/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BC%98%E5%8C%96%E7%A4%BA%E4%BE%8B1.png)
